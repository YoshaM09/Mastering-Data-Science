{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P0kIJG9Ei_a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['I love my dog','I love my cat','I am going to move to SF']"
      ],
      "metadata": {
        "id": "QLyGGXOYFx56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words = 100, oov_token = '<OOV>') #It catches the exceptions like special characters"
      ],
      "metadata": {
        "id": "NRgyHBs_GhEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Words are represented as numbers called tokens"
      ],
      "metadata": {
        "id": "po7eQdTBg4vO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thoaDUrREse4",
        "outputId": "c894071b-496c-46a1-ad8c-571daf50d441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'to': 5, 'dog': 6, 'cat': 7, 'am': 8, 'going': 9, 'move': 10, 'sf': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sequences of numbers from sentences"
      ],
      "metadata": {
        "id": "MTBRlmpIg-TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8uK5Hl8gsS5",
        "outputId": "a93a14e9-5544-4b53-ced1-d457af07043d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 3, 4, 6], [2, 3, 4, 7], [2, 8, 9, 5, 10, 5, 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### But it does not work in test data if new words are present so we need OOV_token (out of vocabulary) to handle that."
      ],
      "metadata": {
        "id": "4CjGMLbehakW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences1 = ['I love going for a walk with my dog','I love my cat','I am going to move to SF']"
      ],
      "metadata": {
        "id": "WBbZGWErhZCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences1 = tokenizer.texts_to_sequences(sentences1)\n",
        "print(sequences1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH6NyH1fh4gO",
        "outputId": "c241d20c-7ae7-4db9-b95c-1912a0d1f290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 3, 9, 1, 1, 1, 1, 4, 6], [2, 3, 4, 7], [2, 8, 9, 5, 10, 5, 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To handle sentence with diff sizes - pad_sequences (Pad the sequences- make them same length by adding 0 at the begenning or ending)"
      ],
      "metadata": {
        "id": "As_fGNhhiR_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(sequences1,padding = 'post')\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEFZBkhCiEtu",
        "outputId": "2c7d2827-8ef8-4db6-eaa6-8cef1f9a2995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2  3  9  1  1  1  1  4  6]\n",
            " [ 2  3  4  7  0  0  0  0  0]\n",
            " [ 2  8  9  5 10  5 11  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded2 = pad_sequences(sequences1,padding = 'post',maxlen = 5)\n",
        "print(padded2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFyEA9kwjQJU",
        "outputId": "ff783634-53c8-4db7-c183-cb040d37b49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  1  1  4  6]\n",
            " [ 2  3  4  7  0]\n",
            " [ 9  5 10  5 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded3 = pad_sequences(sequences1,padding = 'post',maxlen = 5, truncating='post')\n",
        "print(padded3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s9tV_IMjd_g",
        "outputId": "263db1be-3c95-442c-e27d-e15010108f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2  3  9  1  1]\n",
            " [ 2  3  4  7  0]\n",
            " [ 2  8  9  5 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classifier to identify sentiment in the text"
      ],
      "metadata": {
        "id": "YtYK13gQj1cz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5xsrSJRj0lb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}